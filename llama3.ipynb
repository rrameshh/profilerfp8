{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roshniramesh/fp8llm/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.53it/s]\n",
      "Decompressing model: 224it [00:20, 10.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-24T16:03:24.635667-0400 | infer_recipe_from_model_path | INFO - Found recipe in the model_path: /Users/roshniramesh/.cache/huggingface/hub/models--neuralmagic--Meta-Llama-3.1-8B-FP8/snapshots/91994a9a9b33939a9cc0a15b8f2a1f5aafb68aef/recipe.yaml\n",
      "2024-09-24T16:03:24.636568-0400 | _check_create_state | INFO - State created for compression lifecycle\n",
      "2024-09-24T16:03:24.640208-0400 | create_instance | INFO - Loading recipe from file /Users/roshniramesh/.cache/huggingface/hub/models--neuralmagic--Meta-Llama-3.1-8B-FP8/snapshots/91994a9a9b33939a9cc0a15b8f2a1f5aafb68aef/recipe.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-24T16:03:24.695501-0400 | _check_compile_recipe | INFO - Recipe compiled and 1 modifiers created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "manager stage: Model structure initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-24T16:03:24.698864-0400 | pre_initialize_structure | INFO - Compression lifecycle structure pre-initialized for 1 modifiers\n",
      "2024-09-24T16:03:24.699185-0400 | initialize_recipe | INFO - Applied an unstaged recipe to the model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from llmcompressor.transformers import SparseAutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "model_stub = \"neuralmagic/Meta-Llama-3.1-8B-FP8\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_stub)\n",
    "\n",
    "# Load the model\n",
    "model = SparseAutoModelForCausalLM.from_pretrained(model_stub, torch_dtype=torch.float16)\n",
    "\n",
    "weights = model.state_dict()\n",
    "\n",
    "# # Extract weights for attention and MLP layers\n",
    "# selfattn_weights = {}\n",
    "attention_weights = {}\n",
    "mlp_weights = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n"
     ]
    }
   ],
   "source": [
    "temp = {}\n",
    "for name, param in weights.items():\n",
    "    if any(layer in name for layer in ['.0', '15', '31']) and (\n",
    "        name.endswith('self_attn.q_proj.weight') or \n",
    "        name.endswith('self_attn.k_proj.weight') or \n",
    "        name.endswith('self_attn.v_proj.weight') or \n",
    "        name.endswith('self_attn.o_proj.weight')\n",
    "    ):\n",
    "        # temp[name] = param.to(dtype=torch.float8_e4m3fn)\n",
    "        temp[name] = param\n",
    "        print(temp[name].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zerolayer = {name: weight for name, weight in temp.items() if any(x in name for x in [\"0\"])}\n",
    "print(zerolayer.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiflayer = {name: weight for name, weight in temp.items() if any(x in name for x in [\"15\"])}\n",
    "print(fiflayer.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirlayer = {name: weight for name, weight in temp.items() if any(x in name for x in [\"31\"])}\n",
    "print(thirlayer.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'g', 'b', 'c']\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "num_bins = 10\n",
    "bar_width = 0.2  \n",
    "\n",
    "x_positions = np.arange(num_bins)\n",
    "\n",
    "for i, (layer_name, weight) in enumerate(thirlayer.items()):\n",
    "    if weight.dtype == torch.float8_e4m3fn:\n",
    "        weight = weight.to(torch.float32)\n",
    "    weight_np = weight.detach().cpu().numpy()\n",
    "\n",
    "    hist, bin_edges = np.histogram(weight_np.flatten(), bins=num_bins)\n",
    "\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "    plt.bar(x_positions + i * bar_width, hist, width=bar_width,\n",
    "            alpha=0.5, label=layer_name, color=colors[i % len(colors)])\n",
    "\n",
    "tick_positions = x_positions + bar_width * (len(thirlayer) - 1) / 2\n",
    "\n",
    "plt.xticks(tick_positions, \n",
    "           [f'{bin_edges[j]:.2f} - {bin_edges[j+1]:.2f}' for j in range(num_bins)], \n",
    "           rotation=45, ha='right')\n",
    "\n",
    "plt.title('Weight Distributions of Layer 31')\n",
    "plt.xlabel('Weight Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'g', 'b', 'c']\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "num_bins = 10\n",
    "bar_width = 0.2  \n",
    "\n",
    "x_positions = np.arange(num_bins)\n",
    "\n",
    "for i, (layer_name, weight) in enumerate(fiflayer.items()):\n",
    "    if weight.dtype == torch.float8_e4m3fn:\n",
    "        weight = weight.to(torch.float32)\n",
    "    weight_np = weight.detach().cpu().numpy()\n",
    "\n",
    "    hist, bin_edges = np.histogram(weight_np.flatten(), bins=num_bins)\n",
    "\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "    plt.bar(x_positions + i * bar_width, hist, width=bar_width,\n",
    "            alpha=0.5, label=layer_name, color=colors[i % len(colors)])\n",
    "\n",
    "tick_positions = x_positions + bar_width * (len(fiflayer) - 1) / 2\n",
    "\n",
    "plt.xticks(tick_positions, \n",
    "           [f'{bin_edges[j]:.2f} - {bin_edges[j+1]:.2f}' for j in range(num_bins)], \n",
    "           rotation=45, ha='right')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Weight Distributions of Layer 15')\n",
    "plt.xlabel('Weight Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'g', 'b', 'c']\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "num_bins = 10\n",
    "bar_width = 0.2 \n",
    "\n",
    "x_positions = np.arange(num_bins)\n",
    "\n",
    "for i, (layer_name, weight) in enumerate(zerolayer.items()):\n",
    "    if weight.dtype == torch.float8_e4m3fn:\n",
    "        weight = weight.to(torch.float32)\n",
    "    weight_np = weight.detach().cpu().numpy()\n",
    "\n",
    "    hist, bin_edges = np.histogram(weight_np.flatten(), bins=num_bins)\n",
    "\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    # plt.xlim([bin_edges[0], bin_edges[-1]])\n",
    "    plt.bar(x_positions + i * bar_width, hist, width=bar_width,\n",
    "            alpha=0.5, label=layer_name, color=colors[i % len(colors)])\n",
    "\n",
    "tick_positions = x_positions + bar_width * (len(zerolayer) - 1) / 2\n",
    "\n",
    "plt.xticks(tick_positions, \n",
    "           [f'{bin_edges[j]:.2f} - {bin_edges[j+1]:.2f}' for j in range(num_bins)], \n",
    "           rotation=45, ha='right')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Weight Distributions of Layer 0')\n",
    "plt.xlabel('Weight Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp['model.layers.0.self_attn.q_proj.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp['model.layers.0.self_attn.q_proj.weight'].shape)\n",
    "print(temp['model.layers.0.self_attn.q_proj.weight'].size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def extract_exp_mantissa(fp8_tensor):\n",
    "#     # Ensure the tensor is of float8_e4m3fn type\n",
    "#     if fp8_tensor.dtype != torch.float8_e4m3fn:\n",
    "#         raise ValueError(\"Input tensor must be of type torch.float8_e4m3fn\")\n",
    "\n",
    "#     int_repr = fp8_tensor.view(torch.int8)\n",
    "\n",
    "#     # Masks for extracting mantissa and exponent\n",
    "#     mantissa_mask = 0b00000111  # 3 bits mask for mantissa\n",
    "#     exp_mask = 0b00001111       # 4 bits mask for exponent\n",
    "\n",
    "#     # Initialize lists to store extracted mantissas and exponents\n",
    "#     # mantissas = []\n",
    "#     # exponents = []\n",
    "#     max_mantissa = float('-inf')\n",
    "#     corresponding_exponent = None\n",
    "\n",
    "#     # Iterate through each value in the tensor\n",
    "#     for value in int_repr.flatten():\n",
    "#         val = value.item()\n",
    "#         print(val)\n",
    "\n",
    "#         # Extract mantissa and exponent\n",
    "#         mantissa = val & mantissa_mask\n",
    "#         exp = (val >> 3) & exp_mask\n",
    "#         # mantissas.append(mantissa)\n",
    "#         # exponents.append(exp)\n",
    "\n",
    "#         if mantissa > max_mantissa:\n",
    "#             max_mantissa = mantissa\n",
    "#             corresponding_exponent = exp\n",
    "        \n",
    "#         # mantissas.append(mantissa)\n",
    "#         # exponents.append(exp)\n",
    "\n",
    "#         # Print the details (optional)\n",
    "#         binary_str = format(val & 0b11111111, '08b')  # Mask to 8 bits\n",
    "#         # return exp, mantissa\n",
    "#         print(f\"Value: {val} | Binary: {binary_str} | Mantissa: {mantissa} | Exp: {exp}\")\n",
    "\n",
    "#     # maxMan= max(mantissas)\n",
    "#     # maxExp = exponents[mantissas.index(maxMan)]\n",
    "#     # print(mantissas)\n",
    "#     # print(exponents)\n",
    "            \n",
    "#     return max_mantissa, corresponding_exponent\n",
    "#     # return torch.tensor(mantissas), torch.tensor(exponents)\n",
    "\n",
    "# # Example usage\n",
    "# # fp8_tensor = torch.tensor(temp['model.layers.0.self_attn.q_proj.weight'], dtype=torch.float8_e4m3fn)  # Sample FP8 tensor\n",
    "# # mantissas, exponents = extract_exp_mantissa(fp8_tensor)\n",
    "# # print(f\"Mantissas: {mantissas}, Exponents: {exponents}\")\n",
    "# test = torch.tensor([-0.108, 1.44, 0, 0.40625], dtype=torch.float8_e4m3fn)\n",
    "# mantissa, expo = extract_exp_mantissa(test)\n",
    "# print(mantissa)\n",
    "# print(expo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_exp_mantissa(fp8_tensor):\n",
    "    # Ensure the tensor is of float8_e4m3fn type\n",
    "    if fp8_tensor.dtype != torch.float8_e4m3fn:\n",
    "        raise ValueError(\"Input tensor must be of type torch.float8_e4m3fn\")\n",
    "\n",
    "    int_repr = fp8_tensor.view(torch.int8)\n",
    "    int_repr = torch.flatten(int_repr, start_dim=0)\n",
    "\n",
    "\n",
    "    # Masks for extracting mantissa and exponent\n",
    "    mantissa_mask = 0b00000111  # 3 bits mask for mantissa\n",
    "    exp_mask = 0b00001111       # 4 bits mask for exponent\n",
    "\n",
    "    # Extract mantissa and exponent using bitwise operations\n",
    "    mantissas = int_repr & mantissa_mask\n",
    "    exponents = (int_repr >> 3) & exp_mask\n",
    "\n",
    "    # Find the maximum mantissa and the corresponding exponent\n",
    "    # torch.flatten(fp8_tensor)\n",
    "    # int_repr = torch.flatten(int_repr, start_dim=0)\n",
    "    # mantissas = torch.flatten(mantissas, start_dim=0)\n",
    "    # exponents = torch.flatten(exponents, start_dim=0)\n",
    "    \n",
    "    # print(f\"int_rep: {int_repr.shape}\\tmantissas: {mantissas.shape}\\texponents:{exponents.shape}\")\n",
    "    print(mantissas)\n",
    "    print(exponents)\n",
    "    max_mantissa, max_idx = torch.max(mantissas, dim=0)  # Use dim=0 for 1D result across columns\n",
    "    # max_mantissa = max_mantissa.max(dim=0)\n",
    "    corresponding_exponent = exponents[max_idx]\n",
    "\n",
    "\n",
    "    return max_mantissa, corresponding_exponent  \n",
    "\n",
    "# test = torch.tensor([-0.108, 1.44, 0, 0.40625], dtype=torch.float8_e4m3fn)\n",
    "# mantissa, expo = extract_exp_mantissa(test)\n",
    "# print(mantissa)\n",
    "# print(expo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = temp['model.layers.0.self_attn.q_proj.weight']\n",
    "weights = weights[0:16,0:16]\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 0, 3, 1, 2, 6, 0, 1, 5, 6, 6, 0, 2, 1, 5, 3, 7, 1, 3, 5, 4, 2, 0, 1,\n",
      "        4, 1, 2, 1, 1, 4, 4, 0, 1, 2, 1, 2, 1, 0, 0, 1, 0, 3, 1, 0, 2, 0, 4, 1,\n",
      "        1, 0, 1, 4, 7, 3, 0, 0, 1, 1, 3, 0, 1, 0, 1, 1, 6, 3, 2, 5, 1, 6, 0, 1,\n",
      "        2, 6, 0, 1, 3, 0, 6, 1, 1, 4, 4, 6, 0, 7, 0, 1, 4, 2, 1, 1, 3, 4, 4, 1,\n",
      "        3, 3, 5, 3, 3, 0, 0, 0, 4, 0, 0, 2, 1, 3, 6, 0, 0, 2, 6, 3, 0, 4, 0, 1,\n",
      "        6, 7, 1, 0, 0, 4, 5, 0, 0, 0, 0, 4, 6, 4, 0, 1, 4, 3, 1, 1, 2, 5, 1, 2,\n",
      "        3, 0, 6, 5, 1, 7, 0, 0, 6, 3, 1, 1, 1, 5, 7, 2, 3, 1, 0, 5, 1, 4, 0, 1,\n",
      "        0, 3, 0, 1, 0, 3, 0, 1, 1, 4, 1, 0, 4, 3, 0, 1, 3, 1, 0, 0, 2, 0, 2, 1,\n",
      "        2, 4, 0, 2, 0, 1, 0, 2, 2, 7, 5, 1, 2, 2, 3, 1, 2, 0, 2, 3, 0, 0, 0, 2,\n",
      "        1, 6, 1, 2, 3, 5, 3, 2, 1, 4, 1, 1, 6, 6, 0, 0, 1, 5, 0, 1, 1, 3, 1, 0,\n",
      "        0, 3, 4, 1, 1, 2, 0, 2, 3, 3, 1, 2, 3, 3, 1, 2], dtype=torch.int8)\n",
      "tensor([0, 2, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 2, 0, 0, 0, 1, 0, 1, 2, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "        1, 3, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0, 0, 0,\n",
      "        1, 0, 2, 0, 0, 1, 1, 0, 3, 3, 1, 1, 0, 0, 0, 0, 2, 1, 3, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 2, 0, 0, 0, 1, 2, 2, 0, 0, 1, 0, 0, 1, 3, 3, 1, 2, 0, 0, 0,\n",
      "        2, 0, 0, 0, 0, 0, 1, 0, 0, 2, 3, 1, 2, 1, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0,\n",
      "        0, 3, 2, 1, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 1, 0, 0,\n",
      "        1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 2, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], dtype=torch.int8)\n",
      "tensor(7, dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "def max_convolution(weights, window_size=(16, 16)):\n",
    "    w_height, w_width = weights.shape\n",
    "    k_height, k_width = window_size\n",
    "\n",
    "    # Initialize an output tensor to store the max mantissas\n",
    "    output_shape = (w_height - k_height + 1, w_width - k_width + 1)\n",
    "    max_mantissas_output = torch.zeros(output_shape, dtype=torch.int8)\n",
    "    cor_exp_output = torch.zeros(output_shape, dtype=torch.int8)  # If you plan to use it later\n",
    "\n",
    "    for i in range(output_shape[0]):\n",
    "        for j in range(output_shape[1]):\n",
    "            # Define the current window\n",
    "            current_window = weights[i:i + k_height, j:j + k_width]\n",
    "            \n",
    "            # Extract mantissas from the current window\n",
    "            max_mantissa, exp = extract_exp_mantissa(current_window)\n",
    "            print(max_mantissa)\n",
    "            # print(max_mantissa)\n",
    "            # print(exp)\n",
    "            # max_val, max_ind = max_mantissa.max(dim=0)  # Max along the correct dimension\n",
    "            \n",
    "            # Store the max mantissa in the output tensor\n",
    "            max_mantissas_output[i, j] = max_mantissa.item()\n",
    "\n",
    "            cor_exp_output[i, j] = exp.item()  # Get the corresponding exponent\n",
    "            # x = x+1\n",
    "\n",
    "    return max_mantissas_output, cor_exp_output\n",
    "\n",
    "# Example usage\n",
    "weights = temp['model.layers.0.self_attn.q_proj.weight']\n",
    "# weights = weights[0:16, 0:16]\n",
    "\n",
    "window_size = (16, 16)  # Define your window size\n",
    "mantissas,exps = max_convolution(weights, window_size)\n",
    "\n",
    "# Save results\n",
    "mantissas_flat = mantissas.numpy().flatten()\n",
    "exps_flat = exps.numpy().flatten()\n",
    "\n",
    "with open('result.txt', 'w') as f:\n",
    "    for value, exp in zip(mantissas_flat, exps_flat):  # Use zip to iterate over both arrays\n",
    "        f.write(f\"{exp} {value}\\n\")  # Write each value on a new line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tabulate import tabulate\n",
    " \n",
    "f32_type = torch.float32\n",
    "bf16_type = torch.bfloat16\n",
    "e4m3_type = torch.float8_e4m3fn\n",
    "e5m2_type = torch.float8_e5m2\n",
    "\n",
    "# collect finfo for each type\n",
    "table = []\n",
    "for dtype in [f32_type, bf16_type, e4m3_type, e5m2_type]:\n",
    "    numbits = 32 if dtype == f32_type else 16 if dtype == bf16_type else 8\n",
    "    info = torch.finfo(dtype)\n",
    "    table.append([info.dtype, numbits, info.max, \n",
    "                  info.min, info.smallest_normal, info.eps])\n",
    "\n",
    "headers = ['data type', 'bits', 'max', 'min', 'smallest normal', 'eps']\n",
    "print(tabulate(table, headers=headers))\n",
    " \n",
    "'''\n",
    "Output:\n",
    "\n",
    "data type      bits          max           min  smallest normal          eps\n",
    "-------------  ----  -----------  ------------  ---------------  -----------\n",
    "float32          32  3.40282e+38  -3.40282e+38      1.17549e-38  1.19209e-07\n",
    "bfloat16         16  3.38953e+38  -3.38953e+38      1.17549e-38    0.0078125\n",
    "float8_e4m3fn     8          448          -448         0.015625        0.125\n",
    "float8_e5m2       8        57344        -57344      6.10352e-05         0.25\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
