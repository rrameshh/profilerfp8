{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.98it/s]\n",
      "Decompressing model: 224it [00:27,  8.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-24T17:08:29.853697-0400 | infer_recipe_from_model_path | INFO - Found recipe in the model_path: /Users/roshniramesh/.cache/huggingface/hub/models--neuralmagic--Meta-Llama-3.1-8B-FP8/snapshots/91994a9a9b33939a9cc0a15b8f2a1f5aafb68aef/recipe.yaml\n",
      "2024-09-24T17:08:29.860002-0400 | create_instance | INFO - Loading recipe from file /Users/roshniramesh/.cache/huggingface/hub/models--neuralmagic--Meta-Llama-3.1-8B-FP8/snapshots/91994a9a9b33939a9cc0a15b8f2a1f5aafb68aef/recipe.yaml\n",
      "2024-09-24T17:08:29.864081-0400 | _check_compile_recipe | INFO - Recipe compiled and 3 modifiers created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "manager stage: Model structure initialized\n",
      "manager stage: Model structure initialized\n",
      "manager stage: Model structure initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-24T17:08:29.875469-0400 | pre_initialize_structure | INFO - Compression lifecycle structure pre-initialized for 3 modifiers\n",
      "2024-09-24T17:08:29.875815-0400 | initialize_recipe | INFO - Applied a staged recipe with 3 stages to the model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from llmcompressor.transformers import SparseAutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "model_stub = \"neuralmagic/Meta-Llama-3.1-8B-FP8\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_stub)\n",
    "\n",
    "# Load the model\n",
    "model = SparseAutoModelForCausalLM.from_pretrained(model_stub, torch_dtype=torch.float16)\n",
    "\n",
    "weights = model.state_dict()\n",
    "\n",
    "# # Extract weights for attention and MLP layers\n",
    "# selfattn_weights = {}\n",
    "attention_weights = {}\n",
    "mlp_weights = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n"
     ]
    }
   ],
   "source": [
    "temp = {}\n",
    "for name, param in weights.items():\n",
    "    if any(layer in name for layer in ['.0', '15', '31']) and (\n",
    "        name.endswith('self_attn.q_proj.weight') or \n",
    "        name.endswith('self_attn.k_proj.weight') or \n",
    "        name.endswith('self_attn.v_proj.weight') or \n",
    "        name.endswith('self_attn.o_proj.weight')\n",
    "    ):\n",
    "        # temp[name] = param.to(dtype=torch.float8_e4m3fn)\n",
    "        temp[name] = param\n",
    "        print(temp[name].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n",
      "torch.float8_e4m3fn\n"
     ]
    }
   ],
   "source": [
    "mlps = {}\n",
    "if isinstance(weights, dict):\n",
    "    for name, param in weights.items():\n",
    "        if any(layer in name for layer in ['.0', '15', '31']) and 'mlp' in name.lower():\n",
    "            mlps[name] = param\n",
    "            print(mlps[name].dtype)\n",
    "else:\n",
    "    print(\"Weights is not a dictionary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.mlp.gate_proj.weight\n",
      "model.layers.0.mlp.gate_proj.input_scale\n",
      "model.layers.0.mlp.gate_proj.input_zero_point\n",
      "model.layers.0.mlp.gate_proj.weight_scale\n",
      "model.layers.0.mlp.gate_proj.weight_zero_point\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "model.layers.0.mlp.up_proj.input_scale\n",
      "model.layers.0.mlp.up_proj.input_zero_point\n",
      "model.layers.0.mlp.up_proj.weight_scale\n",
      "model.layers.0.mlp.up_proj.weight_zero_point\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "model.layers.0.mlp.down_proj.input_scale\n",
      "model.layers.0.mlp.down_proj.input_zero_point\n",
      "model.layers.0.mlp.down_proj.weight_scale\n",
      "model.layers.0.mlp.down_proj.weight_zero_point\n",
      "model.layers.15.mlp.gate_proj.weight\n",
      "model.layers.15.mlp.gate_proj.input_scale\n",
      "model.layers.15.mlp.gate_proj.input_zero_point\n",
      "model.layers.15.mlp.gate_proj.weight_scale\n",
      "model.layers.15.mlp.gate_proj.weight_zero_point\n",
      "model.layers.15.mlp.up_proj.weight\n",
      "model.layers.15.mlp.up_proj.input_scale\n",
      "model.layers.15.mlp.up_proj.input_zero_point\n",
      "model.layers.15.mlp.up_proj.weight_scale\n",
      "model.layers.15.mlp.up_proj.weight_zero_point\n",
      "model.layers.15.mlp.down_proj.weight\n",
      "model.layers.15.mlp.down_proj.input_scale\n",
      "model.layers.15.mlp.down_proj.input_zero_point\n",
      "model.layers.15.mlp.down_proj.weight_scale\n",
      "model.layers.15.mlp.down_proj.weight_zero_point\n",
      "model.layers.31.mlp.gate_proj.weight\n",
      "model.layers.31.mlp.gate_proj.input_scale\n",
      "model.layers.31.mlp.gate_proj.input_zero_point\n",
      "model.layers.31.mlp.gate_proj.weight_scale\n",
      "model.layers.31.mlp.gate_proj.weight_zero_point\n",
      "model.layers.31.mlp.up_proj.weight\n",
      "model.layers.31.mlp.up_proj.input_scale\n",
      "model.layers.31.mlp.up_proj.input_zero_point\n",
      "model.layers.31.mlp.up_proj.weight_scale\n",
      "model.layers.31.mlp.up_proj.weight_zero_point\n",
      "model.layers.31.mlp.down_proj.weight\n",
      "model.layers.31.mlp.down_proj.input_scale\n",
      "model.layers.31.mlp.down_proj.input_zero_point\n",
      "model.layers.31.mlp.down_proj.weight_scale\n",
      "model.layers.31.mlp.down_proj.weight_zero_point\n"
     ]
    }
   ],
   "source": [
    "for key in mlps.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zerolayer = {name: weight for name, weight in temp.items() if any(x in name for x in [\"0\"])}\n",
    "print(zerolayer.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiflayer = {name: weight for name, weight in temp.items() if any(x in name for x in [\"15\"])}\n",
    "print(fiflayer.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirlayer = {name: weight for name, weight in temp.items() if any(x in name for x in [\"31\"])}\n",
    "print(thirlayer.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'g', 'b', 'c']\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "num_bins = 10\n",
    "bar_width = 0.2  \n",
    "\n",
    "x_positions = np.arange(num_bins)\n",
    "\n",
    "for i, (layer_name, weight) in enumerate(thirlayer.items()):\n",
    "    if weight.dtype == torch.float8_e4m3fn:\n",
    "        weight = weight.to(torch.float32)\n",
    "    weight_np = weight.detach().cpu().numpy()\n",
    "\n",
    "    hist, bin_edges = np.histogram(weight_np.flatten(), bins=num_bins)\n",
    "\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "    plt.bar(x_positions + i * bar_width, hist, width=bar_width,\n",
    "            alpha=0.5, label=layer_name, color=colors[i % len(colors)])\n",
    "\n",
    "tick_positions = x_positions + bar_width * (len(thirlayer) - 1) / 2\n",
    "\n",
    "plt.xticks(tick_positions, \n",
    "           [f'{bin_edges[j]:.2f} - {bin_edges[j+1]:.2f}' for j in range(num_bins)], \n",
    "           rotation=45, ha='right')\n",
    "\n",
    "plt.title('Weight Distributions of Layer 31')\n",
    "plt.xlabel('Weight Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'g', 'b', 'c']\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "num_bins = 10\n",
    "bar_width = 0.2  \n",
    "\n",
    "x_positions = np.arange(num_bins)\n",
    "\n",
    "for i, (layer_name, weight) in enumerate(fiflayer.items()):\n",
    "    if weight.dtype == torch.float8_e4m3fn:\n",
    "        weight = weight.to(torch.float32)\n",
    "    weight_np = weight.detach().cpu().numpy()\n",
    "\n",
    "    hist, bin_edges = np.histogram(weight_np.flatten(), bins=num_bins)\n",
    "\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "    plt.bar(x_positions + i * bar_width, hist, width=bar_width,\n",
    "            alpha=0.5, label=layer_name, color=colors[i % len(colors)])\n",
    "\n",
    "tick_positions = x_positions + bar_width * (len(fiflayer) - 1) / 2\n",
    "\n",
    "plt.xticks(tick_positions, \n",
    "           [f'{bin_edges[j]:.2f} - {bin_edges[j+1]:.2f}' for j in range(num_bins)], \n",
    "           rotation=45, ha='right')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Weight Distributions of Layer 15')\n",
    "plt.xlabel('Weight Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'g', 'b', 'c']\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "num_bins = 10\n",
    "bar_width = 0.2 \n",
    "\n",
    "x_positions = np.arange(num_bins)\n",
    "\n",
    "for i, (layer_name, weight) in enumerate(zerolayer.items()):\n",
    "    if weight.dtype == torch.float8_e4m3fn:\n",
    "        weight = weight.to(torch.float32)\n",
    "    weight_np = weight.detach().cpu().numpy()\n",
    "\n",
    "    hist, bin_edges = np.histogram(weight_np.flatten(), bins=num_bins)\n",
    "\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    # plt.xlim([bin_edges[0], bin_edges[-1]])\n",
    "    plt.bar(x_positions + i * bar_width, hist, width=bar_width,\n",
    "            alpha=0.5, label=layer_name, color=colors[i % len(colors)])\n",
    "\n",
    "tick_positions = x_positions + bar_width * (len(zerolayer) - 1) / 2\n",
    "\n",
    "plt.xticks(tick_positions, \n",
    "           [f'{bin_edges[j]:.2f} - {bin_edges[j+1]:.2f}' for j in range(num_bins)], \n",
    "           rotation=45, ha='right')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Weight Distributions of Layer 0')\n",
    "plt.xlabel('Weight Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp['model.layers.0.self_attn.q_proj.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp['model.layers.0.self_attn.q_proj.weight'].shape)\n",
    "print(temp['model.layers.0.self_attn.q_proj.weight'].size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_exp_mantissa(fp8_tensor):\n",
    "    # Ensure the tensor is of float8_e4m3fn type\n",
    "    if fp8_tensor.dtype != torch.float8_e4m3fn:\n",
    "        raise ValueError(\"Input tensor must be of type torch.float8_e4m3fn\")\n",
    "\n",
    "    int_repr = fp8_tensor.view(torch.int8)\n",
    "    int_repr = torch.flatten(int_repr, start_dim=0)\n",
    "\n",
    "\n",
    "    # Masks for extracting mantissa and exponent\n",
    "    mantissa_mask = 0b00000111  # 3 bits mask for mantissa\n",
    "    exp_mask = 0b00001111       # 4 bits mask for exponent\n",
    "\n",
    "    # Extract mantissa and exponent using bitwise operations\n",
    "    mantissas = int_repr & mantissa_mask\n",
    "    exponents = (int_repr >> 3) & exp_mask\n",
    "\n",
    "    # Find the maximum mantissa and the corresponding exponent\n",
    "    # torch.flatten(fp8_tensor)\n",
    "    # int_repr = torch.flatten(int_repr, start_dim=0)\n",
    "    # mantissas = torch.flatten(mantissas, start_dim=0)\n",
    "    # exponents = torch.flatten(exponents, start_dim=0)\n",
    "    \n",
    "    # print(f\"int_rep: {int_repr.shape}\\tmantissas: {mantissas.shape}\\texponents:{exponents.shape}\")\n",
    "    # print(mantissas)\n",
    "    # print(exponents)\n",
    "    max_mantissa, max_idx = torch.max(mantissas, dim=0)  # Use dim=0 for 1D result across columns\n",
    "    # max_mantissa = max_mantissa.max(dim=0)\n",
    "    corresponding_exponent = exponents[max_idx]\n",
    "\n",
    "\n",
    "    return max_mantissa, corresponding_exponent  \n",
    "\n",
    "# test = torch.tensor([-0.108, 1.44, 0, 0.40625], dtype=torch.float8_e4m3fn)\n",
    "# mantissa, expo = extract_exp_mantissa(test)\n",
    "# print(mantissa)\n",
    "# print(expo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = temp['model.layers.0.self_attn.q_proj.weight']\n",
    "weights = weights[0:16,0:16]\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_convolution(weights, window_size=(16, 16)):\n",
    "    w_height, w_width = weights.shape\n",
    "    k_height, k_width = window_size\n",
    "\n",
    "    output_shape = (w_height - k_height + 1, w_width - k_width + 1)\n",
    "    max_mantissas_output = torch.zeros(output_shape, dtype=torch.int8)\n",
    "    cor_exp_output = torch.zeros(output_shape, dtype=torch.int8)  # If you plan to use it later\n",
    "\n",
    "    for i in range(output_shape[0]):\n",
    "        for j in range(output_shape[1]):\n",
    "            # Define the current window\n",
    "            current_window = weights[i:i + k_height, j:j + k_width]\n",
    "            \n",
    "            # Extract mantissas from the current window\n",
    "            max_mantissa, exp = extract_exp_mantissa(current_window)\n",
    "            # print(max_mantissa)\n",
    "            # print(max_mantissa)\n",
    "            # print(exp)\n",
    "            # max_val, max_ind = max_mantissa.max(dim=0) \n",
    "            \n",
    "         \n",
    "            max_mantissas_output[i, j] = max_mantissa.item()\n",
    "\n",
    "            cor_exp_output[i, j] = exp.item()  \n",
    "            # x = x+1\n",
    "\n",
    "    return max_mantissas_output, cor_exp_output\n",
    "\n",
    "# Example usage\n",
    "model = 'model.layers.0.self_attn.q_proj.weight'\n",
    "weights = temp[model]\n",
    "# weights = weights[0:16, 0:16]\n",
    "\n",
    "window_size = (16, 16)  \n",
    "mantissas,exps = max_convolution(weights, window_size)\n",
    "\n",
    "# Save results\n",
    "mantissas_flat = mantissas.numpy().flatten()\n",
    "exps_flat = exps.numpy().flatten()\n",
    "\n",
    "with open('result.txt', 'w') as f:\n",
    "    for value, exp in zip(mantissas_flat, exps_flat): \n",
    "        f.write(f\"{exp} {value}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tabulate import tabulate\n",
    " \n",
    "f32_type = torch.float32\n",
    "bf16_type = torch.bfloat16\n",
    "e4m3_type = torch.float8_e4m3fn\n",
    "e5m2_type = torch.float8_e5m2\n",
    "\n",
    "# collect finfo for each type\n",
    "table = []\n",
    "for dtype in [f32_type, bf16_type, e4m3_type, e5m2_type]:\n",
    "    numbits = 32 if dtype == f32_type else 16 if dtype == bf16_type else 8\n",
    "    info = torch.finfo(dtype)\n",
    "    table.append([info.dtype, numbits, info.max, \n",
    "                  info.min, info.smallest_normal, info.eps])\n",
    "\n",
    "headers = ['data type', 'bits', 'max', 'min', 'smallest normal', 'eps']\n",
    "print(tabulate(table, headers=headers))\n",
    " \n",
    "'''\n",
    "Output:\n",
    "\n",
    "data type      bits          max           min  smallest normal          eps\n",
    "-------------  ----  -----------  ------------  ---------------  -----------\n",
    "float32          32  3.40282e+38  -3.40282e+38      1.17549e-38  1.19209e-07\n",
    "bfloat16         16  3.38953e+38  -3.38953e+38      1.17549e-38    0.0078125\n",
    "float8_e4m3fn     8          448          -448         0.015625        0.125\n",
    "float8_e5m2       8        57344        -57344      6.10352e-05         0.25\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
